{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_data=pd.read_csv('airline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_name</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>author_country</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>aircraft</th>\n",
       "      <th>type_traveller</th>\n",
       "      <th>cabin_flown</th>\n",
       "      <th>route</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>seat_comfort_rating</th>\n",
       "      <th>cabin_staff_rating</th>\n",
       "      <th>food_beverages_rating</th>\n",
       "      <th>inflight_entertainment_rating</th>\n",
       "      <th>ground_service_rating</th>\n",
       "      <th>wifi_connectivity_rating</th>\n",
       "      <th>value_money_rating</th>\n",
       "      <th>recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adria-airways</td>\n",
       "      <td>/airline-reviews/adria-airways</td>\n",
       "      <td>Adria Airways customer review</td>\n",
       "      <td>D Ito</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2015-04-10</td>\n",
       "      <td>Outbound flight FRA/PRN A319. 2 hours 10 min f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Economy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline_name                            link  \\\n",
       "0  adria-airways  /airline-reviews/adria-airways   \n",
       "\n",
       "                           title author author_country        date  \\\n",
       "0  Adria Airways customer review  D Ito        Germany  2015-04-10   \n",
       "\n",
       "                                             content aircraft type_traveller  \\\n",
       "0  Outbound flight FRA/PRN A319. 2 hours 10 min f...      NaN            NaN   \n",
       "\n",
       "  cabin_flown route  overall_rating  seat_comfort_rating  cabin_staff_rating  \\\n",
       "0     Economy   NaN             7.0                  4.0                 4.0   \n",
       "\n",
       "   food_beverages_rating  inflight_entertainment_rating  \\\n",
       "0                    4.0                            0.0   \n",
       "\n",
       "   ground_service_rating  wifi_connectivity_rating  value_money_rating  \\\n",
       "0                    NaN                       NaN                 4.0   \n",
       "\n",
       "   recommended  \n",
       "0            1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=airline_data.content.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, Tok found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4181c8c3f84a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mtokenize_reviews_no_stop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, Tok found"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Remove Numbers\n",
    "reviews1=[]\n",
    "for i in reviews:\n",
    "    reviews1.append(re.sub(r'\\d+', '', i))\n",
    "    \n",
    "# Remove Punctuation\n",
    "reviews2=[]\n",
    "for i in reviews1:\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    reviews2.append(i.translate(table))\n",
    "    \n",
    "# Remove Whitespaces\n",
    "reviews3=[]\n",
    "for i in reviews2:\n",
    "    reviews3.append(i.strip())\n",
    "    \n",
    "# Removing Stop Words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "tokenize_reviews_no_stop_words=[]\n",
    "for i in reviews3:\n",
    "    tokens=tokenize(i)\n",
    "    result=[j for j in tokens if not j in stop_words]\n",
    "    temp=' '.join(result)\n",
    "    tokenize_reviews_no_stop_words.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(sentence):\n",
    "    result=re.sub(r'\\d+','',sentence)\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    result_new=result.translate(table)\n",
    "    result_new=result_new.strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens=word_tokenize(result_new)\n",
    "    final=[j for j in tokens if not j in stop_words]\n",
    "    temp=' '.join(final)\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1fd90b8fd68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "polarity=[]\n",
    "numlist=[]\n",
    "num=1\n",
    "for i in tokenize_reviews_no_stop_words:\n",
    "    temp=TextBlob(i).sentiment.polarity\n",
    "    polarity.append(temp)\n",
    "    numlist.append(num)\n",
    "    num=num+1\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.scatter(numlist,polarity,label='Polarity')\n",
    "plt.xlabel('Reviews')\n",
    "plt.ylabel('Polarity')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment=[]\n",
    "for i in polarity:\n",
    "    if i==0:\n",
    "        sentiment.append(0)\n",
    "    elif i<0:\n",
    "        sentiment.append(-1)\n",
    "    elif i>0:\n",
    "        sentiment.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=[]\n",
    "for i in range(len(sentiment)):\n",
    "    combined.append([tokenize_reviews_no_stop_words[i],sentiment[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>outbound flight fraprn hours min flight though...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>two short hops zrhlju ljuvie fast crj seats co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flew zurichljubljana jp newish crj flight almo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adria serves min flight ljubljana amsterdam br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wawskj economy free snacks drinks star allianc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  Sentiment\n",
       "0  outbound flight fraprn hours min flight though...          1\n",
       "1  two short hops zrhlju ljuvie fast crj seats co...          1\n",
       "2  flew zurichljubljana jp newish crj flight almo...          1\n",
       "3  adria serves min flight ljubljana amsterdam br...          1\n",
       "4  wawskj economy free snacks drinks star allianc...          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(combined,columns=['Reviews','Sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "df=shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9113201209271078"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression()\n",
    "vectorizer=CountVectorizer(ngram_range=(1,2))\n",
    "x_l=vectorizer.fit_transform(df.Reviews)\n",
    "model.fit(x_l[:29488],df.Sentiment[:29488].values)\n",
    "model.score(x_l[29488:],df.Sentiment[29488:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('logisticModel0.1.pkl', 'wb'))\n",
    "pickle.dump(vectorizer,open('logisticVectorizer0.1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=1e-05,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=42, tol=1e-5)\n",
    "clf.fit(x_l[:29488],df.Sentiment[:29488].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9114040980853207"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_l[29488:],df.Sentiment[29488:])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "t=preprocess_review('I fly at least once a month ZRH-LJU-ZRH. My experience is that the staff on the ground need a better education regarding how to care of passengers. If it comes to a delay and pax have to go to the info desk to change tickets and so on then you really see of how unfriendly and unprepared staff are. The airplanes could be cleaned better and I do also see some room for improvement for the flight staff.')\n",
    "te=vectorizer.transform([t])\n",
    "s=CalibratedClassifierCV(clf)\n",
    "s.fit(x_l[:29488],df.Sentiment[:29488].values)\n",
    "s.predict(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9089687604971448"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.score(x_l[29488:],df.Sentiment[29488:])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(s,open('svmModel0.1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Conv1D,MaxPool1D,Embedding,Flatten,Dropout,GRU,LSTM, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8ea6e878de7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreviews_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msentiment_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "reviews_array=np.array(df.Reviews)\n",
    "sentiment_array=np.array(df.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4fc2d5aca138>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# encode class values as integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mencoded_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# convert integers to dummy variables (i.e. one hot encoded)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment_array' is not defined"
     ]
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(sentiment_array)\n",
    "encoded_Y = encoder.transform(sentiment_array)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=Tokenizer()\n",
    "token.fit_on_texts(reviews_array) \n",
    "vocab_size=len(token.word_index) +1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for i in reviews_array:\n",
    "  l += len(i)\n",
    "  \n",
    "avg_length = l/len(reviews_array)\n",
    "review_training = [x[:int(avg_length)] for x in reviews_array]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = token.texts_to_sequences(reviews_array)\n",
    "l = []\n",
    "for i in encoded:\n",
    "  l.append(len(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs = pad_sequences(encoded, maxlen=38, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,   421,  7196, ...,    75,   530,   458],\n",
       "       [   72,   108,  4368, ...,     0,     0,     0],\n",
       "       [   25, 17571, 14760, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  385,  3650,   867, ...,     0,     0,     0],\n",
       "       [48801, 23410,   241, ...,   618,   564,   426],\n",
       "       [  488,    88,   515, ...,    52,   142,     1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_model1 = Sequential()\n",
    "n_model1.add(Embedding(48802,64,input_length=38))\n",
    "n_model1.add(LSTM(64, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=False))\n",
    "n_model1.add(Dropout(0.5))\n",
    "n_model1.add(LSTM(64,return_sequences=False))\n",
    "n_model1.add(Dropout(0.5))\n",
    "# n_model1.add(Dense(1,activation=\"sigmoid\"))\n",
    "# n_model1.summary()\n",
    "# n_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.add(Dense(8, input_dim=1, activation='relu'))\n",
    "\n",
    "n_model1.add(Dense(3, activation='softmax'))\n",
    "n_model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6d5fa45e9409>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_model1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_docs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdummy_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'n_model1' is not defined"
     ]
    }
   ],
   "source": [
    "hist = n_model1.fit(padded_docs,dummy_y,epochs=5,batch_size=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.995233e-01, 5.564765e-05, 4.210277e-04]], dtype=float32)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded1 = token.texts_to_sequences(['I am feeling very sad and pathetic.'])\n",
    "padded_docs1 = pad_sequences(encoded1, maxlen=38, padding='post')\n",
    "n_model1.predict(padded_docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# reviews=df.Reviews.values\n",
    "# sentiments=df.Sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_train,reviews_test,sentiments_train,sentiments_test=train_test_split(reviews, sentiments, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v=CountVectorizer()\n",
    "# v.fit(reviews_train)\n",
    "# X_train=v.transform(reviews_train)\n",
    "# X_test=v.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-1558f398dfd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodeln\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodeln\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodeln\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1252\u001b[1;33m                 \u001b[1;34m'This model has not yet been built. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m                 \u001b[1;34m'Build the model first by calling build() '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m                 \u001b[1;34m'or calling fit() with some data. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. "
     ]
    }
   ],
   "source": [
    "# input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "# modeln = Sequential()  \n",
    "# #model4.add(Embedding(48802,64,input_length=38))\n",
    "# modeln.add(Bidirectional( LSTM(64 ,dropout_U = 0.2, dropout_W = 0.2)))\n",
    "# modeln.add(Dense(3,activation=\"softmax\"))\n",
    "# modeln.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# modeln.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras import layers\n",
    "\n",
    "# input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "# # model11 = Sequential()\n",
    "# # model11.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# # model11.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model11.compile(loss='binary_crossentropy', \n",
    "#               optimizer='adam', \n",
    "#               metrics=['accuracy'])\n",
    "# model11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model11.fit(X_train, sentiments_train,epochs=10,verbose=True,validation_data=(X_test, sentiments_test),batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model11.evaluate(X_test,sentiments_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t='This is Kunal Verma'\n",
    "# te=v.transform([t])\n",
    "# model11.predict(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kunalverma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 38, 64)            3123328   \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 3,189,763\n",
      "Trainable params: 3,189,763\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()  \n",
    "model4.add(Embedding(48802,64,input_length=38))\n",
    "model4.add(Bidirectional( LSTM(64 ,dropout_U = 0.2, dropout_W = 0.2)))\n",
    "model4.add(Dense(3,activation=\"softmax\"))\n",
    "model4.summary()\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33116 samples, validate on 8280 samples\n",
      "Epoch 1/3\n",
      "33116/33116 [==============================] - 59s 2ms/step - loss: 0.4228 - acc: 0.8262 - val_loss: 0.3817 - val_acc: 0.8300\n",
      "Epoch 2/3\n",
      "33116/33116 [==============================] - 57s 2ms/step - loss: 0.2405 - acc: 0.9042 - val_loss: 0.3759 - val_acc: 0.8339\n",
      "Epoch 3/3\n",
      "33116/33116 [==============================] - 61s 2ms/step - loss: 0.1810 - acc: 0.9299 - val_loss: 0.4352 - val_acc: 0.8362\n"
     ]
    }
   ],
   "source": [
    "hist = model4.fit(padded_docs,dummy_y,epochs=3,batch_size=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09248289, 0.8340015 , 0.07351567]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded1 = token.texts_to_sequences([['Smart and efficient service. Small planes but as a tall man (6ft 4) I did not feel cramped either on the CRJ-200 (AMS-LJU) or the CRJ-900 (LJU-AMS). The cabin crew were friendly and the food for such a short flight was good. Also the price was right.']])\n",
    "padded_docs1 = pad_sequences(encoded1, maxlen=38, padding='post')\n",
    "model4.predict(padded_docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model4.predict_classes(padded_docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.predict_classes(padded_docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41396/41396 [==============================] - 15s 357us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2542341899096891, 0.8955696202531646]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(padded_docs,dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model4,open('lstmModel0.1.pkl','wb'))\n",
    "pickle.dump(token,open('lstmToken.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 38, 100)           4880200   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 38, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 34, 64)            32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 4,913,803\n",
      "Trainable params: 4,913,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Embedding(48802, 100, input_length=38))\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "model_conv.add(MaxPool1D(pool_size=4))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(3, activation='softmax'))\n",
    "model_conv.compile(loss='categorical_crossentropy', optimizer='adam',    metrics=['accuracy'])\n",
    "model_conv.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33116 samples, validate on 8280 samples\n",
      "Epoch 1/10\n",
      "33116/33116 [==============================] - 44s 1ms/step - loss: 0.3541 - acc: 0.8378 - val_loss: 0.3921 - val_acc: 0.8279\n",
      "Epoch 2/10\n",
      "33116/33116 [==============================] - 41s 1ms/step - loss: 0.2209 - acc: 0.9121 - val_loss: 0.3835 - val_acc: 0.8321\n",
      "Epoch 3/10\n",
      "33116/33116 [==============================] - 46s 1ms/step - loss: 0.1342 - acc: 0.9528 - val_loss: 0.4455 - val_acc: 0.8231\n",
      "Epoch 4/10\n",
      "33116/33116 [==============================] - 47s 1ms/step - loss: 0.0604 - acc: 0.9823 - val_loss: 0.5728 - val_acc: 0.8240\n",
      "Epoch 5/10\n",
      "33116/33116 [==============================] - 50s 2ms/step - loss: 0.0220 - acc: 0.9954 - val_loss: 0.7029 - val_acc: 0.8200\n",
      "Epoch 6/10\n",
      "33116/33116 [==============================] - 49s 1ms/step - loss: 0.0084 - acc: 0.9991 - val_loss: 0.7901 - val_acc: 0.8163\n",
      "Epoch 7/10\n",
      "33116/33116 [==============================] - 47s 1ms/step - loss: 0.0035 - acc: 0.9998 - val_loss: 0.8596 - val_acc: 0.8153\n",
      "Epoch 8/10\n",
      "33116/33116 [==============================] - 48s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.9185 - val_acc: 0.8135\n",
      "Epoch 9/10\n",
      "33116/33116 [==============================] - 51s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.9644 - val_acc: 0.8134\n",
      "Epoch 10/10\n",
      "33116/33116 [==============================] - 47s 1ms/step - loss: 7.5611e-04 - acc: 1.0000 - val_loss: 1.0050 - val_acc: 0.8129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fdd2a09358>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(padded_docs,dummy_y, validation_split=0.2, epochs = 10 ,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1023651e-07, 2.7695142e-09, 9.9999976e-01]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded1 = token.texts_to_sequences(['Smart and efficient service. Small planes but as a tall man (6ft 4) I did not feel cramped either on the CRJ-200 (AMS-LJU) or the CRJ-900 (LJU-AMS). The cabin crew were friendly and the food for such a short flight was good. Also the price was right.'])\n",
    "padded_docs1 = pad_sequences(encoded1, maxlen=38, padding='post')\n",
    "model_conv.predict(padded_docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_conv,open('cnnModel0.1.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
